{{ if .Values.enableCalico }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-calico"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
    component: calico
spec:
  groups:
  - name: calico.rules
    rules:
    - alert: CalicoNodeInstanceDown
      expr: up{job="{{ .Release.Name }}-calico"} != 1
      for: 5m
      labels:
        severity: critical
      annotations:
        description: '{{`{{$labels.instance}}`}} of job {{`{{$labels.job}}`}} has been down for more than 5 minutes'
        summary: 'Instance {{`{{$labels.instance}}`}} Pod: {{`{{$labels.pod}}`}} is down'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-caliconodeinstancedown'
    - alert: CalicoDataplaneFailures
      expr: felix_int_dataplane_failures{job="{{ .Release.Name }}-calico"} > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        description: '{{`{{$labels.instance}}`}} with calico-node pod {{`{{$labels.pod}}`}} has been having dataplane failures'
        summary: 'Instance {{`{{$labels.instance}}`}} - Dataplane failures'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-calicodataplanefailures'
{{ end }}

---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-target-critical"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
spec:
  groups:
  - name: target-critical.rules
    rules:
    - alert: InfrastructureTargetDown
      expr: count(up{namespace="infrastructure"} == 0) by (job) / count(up{namespace="infrastructure"}) by (job) > 0.10
      for: 5m
      labels:
        severity: Critical
      annotations:
        message: '{{`{{ $value | humanizePercentage }}`}} of the {{`{{ $labels.job }}`}} targets are down in the infrastructure namespace'
    - alert: SystemTargetDown
      expr: count(up{namespace="kube-system"} == 0) by (job) / count(up{namespace="kube-system"}) by (job) > 0.10
      for: 5m
      labels:
        severity: Critical
      annotations:
        message: '{{`{{ $value | humanizePercentage }}`}} of the {{`{{ $labels.job }}`}} targets are down in the kube-system namespace'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-node-custom"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
spec:
  groups:
  - name: node-custom.rules
    rules:
    - alert: NodeWithImpairedVolumes
      expr: count by(node)(kube_node_spec_taint{key="NodeWithImpairedVolumes", effect="NoSchedule"}) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: 'Node {{`{{$labels.node}}`}} has Impaired Volumes'
        description: 'EBS volumes are failing to attach to node {{`{{$labels.node}}`}}'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-nodewithimpairedvolumes'
    - alert: NodeWithoutEIP
      expr: count by(node)(kube_node_spec_taint{key="node.kubernetes.io/eip-unavailable", effect="NoSchedule"}) > 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: 'No EIP on {{`{{$labels.node}}`}}'
        description: 'Node {{`{{$labels.node}}`}} failed to auto-assign an EIP'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-nodewithouteip'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-certificate-status"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
    group: certificate-status
spec:
  groups:
  - name: certmanager-certificate-status.rules
    rules:
    - alert: CertificateNotReady
      expr: certmanager_certificate_ready_status{condition="True"} != 1
      for: 10m
      labels:
        severity: warning
        group: certificate-status
      annotations:
        message: 'A cert-manager certificate can not be issued/updated'
        description: 'Certificate {{`{{$labels.name}}`}} can not be issued/updated'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-certificate-not-ready'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-certificate-about-to-expire"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
    group: certificate-status
spec:
  groups:
  - name: certmanager-certificate-status.rules
    rules:
    - alert: CertificateAboutToExpire
      expr: certmanager_certificate_expiration_timestamp_seconds < time() + 1209600 # two weeks
      for: 10m
      labels:
        severity: warning
        group: certificate-status
      annotations:
        message: 'A cert-manager certificate is about to expire'
        description: 'Certificate {{`{{$labels.name}}`}} will expire in less than two weeks and has not been renewed'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-certificate-about-to-expire'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-certificate-expiring"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
    group: certificate-status
spec:
  groups:
  - name: certmanager-certificate-status.rules
    rules:
    - alert: CertificateExpiring
      expr: certmanager_certificate_expiration_timestamp_seconds < time() + 604800 # one week
      for: 10m
      labels:
        severity: warning
        group: certificate-status
      annotations:
        message: 'A cert-manager certificate is expiring'
        description: 'Certificate {{`{{$labels.name}}`}} will expire in less than one week and has not been renewed'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-certificate-expiring'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-external-dns-errors"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
    group: external-dns
spec:
  groups:
  - name: external-dns-errors.rules
    rules:
    - alert: ExternalDnsRegistryErrorsIncrease
      expr: increase(external_dns_registry_errors_total{job="external-dns"}[5m]) > 0
      for: 30m
      labels:
        severity: warning
        group: external-dns-errors
      annotations:
        message: External DNS registry Errors increasing constantly
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-externaldnsregistryerrorsincrease'
    - alert: ExternalDNSSourceErrorsIncrease
      expr: increase(external_dns_source_errors_total{job="external-dns"}[5m]) > 0
      for: 30m
      labels:
        severity: warning
        group: external-dns-errors
      annotations:
        message: External DNS source Errors increasing constantly
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-externaldnssourceerrorsincrease'
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-resource-usage"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
spec:
  groups:
  - name: resource-usage.rules
    rules:
    - alert: ContainerExcessiveCPU
      expr: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate) by (container, pod, namespace) / sum(kube_pod_container_resource_requests_cpu_cores) by (container, pod, namespace) > 2.0
      for: 30m
      labels:
        severity: warning
      annotations:
        message: 'Container {{`{{ $labels.container }}`}} of pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been using {{`{{ $value | humanizePercentage }}`}} of it's requested CPU for 30 min'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-containerexcessivecpu'
    - alert: ContainerExcessiveMEM
      expr: sum(container_memory_working_set_bytes) by (container, pod, namespace) / sum(kube_pod_container_resource_requests_memory_bytes) by (container, pod, namespace) > 2.0
      for: 30m
      labels:
        severity: warning
      annotations:
        message: 'Container {{`{{ $labels.container }}`}} of pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} has been using {{`{{ $value | humanizePercentage }}`}} of it's requested Memory for 30min'
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-containerexcessivemem'

{{- if .Values.enableVelero }}
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: "{{ .Release.Name }}-velero"
  namespace: "{{ .Release.Namespace }}"
  labels:
{{ include "labels" $ | indent 4 }}
    prometheus: "{{ .Release.Name }}"
    group: velero
spec:
  groups:
  - name: velero-alerts
    rules:
    - alert: VeleroBackupPartialFailures
      expr: |-
        velero_backup_partial_failure_total{schedule!=""} / velero_backup_attempt_total{schedule!=""} > 0.25
      for: 15m
      labels:
        severity: warning
        group: velero
      annotations:
        message: Velero backup {{`{{ $labels.schedule }}`}} has {{`{{ $value | humanizePercentage }}`}} partial failures.
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-velerobackuppartialfailures'
    - alert: VeleroBackupFailures
      expr: |-
        velero_backup_failure_total{schedule!=""} / velero_backup_attempt_total{schedule!=""} > 0.25
      for: 15m
      labels:
        severity: warning
        group: velero
      annotations:
        message: Velero backup {{`{{ $labels.schedule }}`}} has {{`{{ $value | humanizePercentage }}`}} failures.
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-velerobackupfailures'
    - alert: VeleroVolumeSnapshotFailures
      expr: |-
        velero_volume_snapshot_failure_total{schedule!=""} / velero_volume_snapshot_attempt_total{schedule!=""} > 0.25
      for: 15m
      labels:
        severity: warning
        group: velero
      annotations:
        message: Velero backup {{`{{ $labels.schedule }}`}} has {{`{{ $value | humanizePercentage }}`}} volume snapshot failures.
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-velerovolumesnapshotfailures'
    - alert: VeleroBackupOld
      expr: |-
        velero_backup_last_successful_timestamp{schedule="velero-cluster-backup"} + 129600 < time()
      for: 5m
      labels:
        severity: warning
        group: velero
      annotations:
        message: Cluster hasn't been backed up for more than 36 hours.
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-velerobackupold'
    - alert: VeleroBackupTooOld
      expr: |-
        velero_backup_last_successful_timestamp{schedule="velero-cluster-backup"} + 260000 < time()
      for: 5m
      labels:
        severity: critical
        group: velero
      annotations:
        message: Cluster hasn't been backed up for more than 3 days.
        runbook_url: 'https://github.com/skyscrapers/documentation/tree/master/runbook.md#alert-name-velerobackuptooold'
{{- end }}
